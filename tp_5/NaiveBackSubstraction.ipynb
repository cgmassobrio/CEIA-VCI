{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementación del método de Naive para identificar objetos en movimiento\n",
    "\n",
    "**Índice**   \n",
    "1. [Registro de frame](#id1)\n",
    "2. [Funciones para implementación](#id2)\n",
    "3. [Pruebas por frame](#id3)\n",
    "4. [Segmentación de objetos en movimiento](#id4)\n",
    "5. [Comparativa con métodos de mezclas gaussianas](#id5)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy: 1.22.3\n",
      "openCV: 4.5.5\n",
      "Matplotlib: 3.5.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Librerías principales\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Tipo de visualización\n",
    "%matplotlib inline\n",
    "\n",
    "# Versiones de librerías\n",
    "print(\"\".join(f\"{x[0]}: {x[1]}\\n\" for x in [\n",
    "    (\"Numpy\",np.__version__),\n",
    "    (\"openCV\",cv.__version__),\n",
    "    (\"Matplotlib\",matplotlib.__version__),\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de la ruta para levantar los videos\n",
    "VD_DIR = r'.\\videos'\n",
    "VD_NAME = 'vtest.avi'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registro de frames<a name=\"id1\"></a>\n",
    "Se levanta el video que se empleará como test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación del objeto para luego manipular los frames\n",
    "#-------------------\n",
    "capture = cv.VideoCapture(os.path.join(VD_DIR, VD_NAME))\n",
    "\n",
    "if not capture.isOpened:\n",
    "    print('Falla al abrir el archivo: ' + VD_NAME)\n",
    "    exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada frame se guarda en un array. Se tratará como un tensor. La primera dimensión da cuenta de la cantidad de frame que posee el video y fueron guardados de esta forma. Luego se extraen valores a emplear luego.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La cantidad de frames guardados es correcta\n",
      "Cantidad de frames registrados:  795\n",
      "Verificación de dimensiones:  (795, 576, 768, 3)\n"
     ]
    }
   ],
   "source": [
    "# Guardad de cada frame en array\n",
    "frames = []\n",
    "while True:\n",
    "    read, frame= capture.read()\n",
    "    if not read:\n",
    "        break\n",
    "    frames.append(frame)\n",
    "frames = np.array(frames)\n",
    "\n",
    "# Frames por segundo\n",
    "fps = capture.get(cv.CAP_PROP_FPS)\n",
    "# Cantidad de frames guardados como array\n",
    "cant_frames = len(frames)\n",
    "\n",
    "# Verificación de lectura correcta\n",
    "if int(capture.get(cv.CAP_PROP_FRAME_COUNT)) == len(frames):\n",
    "    print(\"La cantidad de frames guardados es correcta\")\n",
    "    print(\"Cantidad de frames registrados: \", cant_frames)\n",
    "    print(\"Verificación de dimensiones: \", frames.shape)\n",
    "else:\n",
    "    print(\"No se ha guardado correctamente la cantidad de frames\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones para implementación<a name=\"id2\"></a>\n",
    "La implementación se separó en dos partes. La primera es la construcción del background. Se calcula como la mediana de una muestra aleatoria de frames. El cálculo es por canal y por pixel. Luego se arma la máscara o _foreground_. Para ello se toma el frame que se está leyendo en el momento de la reproducción del video. Se la mejora aplicando en pasos:\n",
    "- Binarización tipo Otsu.\n",
    "- Filtro morfológico de cierre, ya que conviene cerrar la figura para que no haya agujeros (ceros) dentro de los objetos que afecten la transformada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def background_naive(input_frames, batch_size, seed, m_type='float32'):\n",
    "    '''\n",
    "    Función que devuelve la mediana, por canal, de un batch random formado por los frames de un video\n",
    "    Los valores son de tipo flotante por defecto.\n",
    "    - input_frames: array que contiene los frames de video a procesar.\n",
    "    - batch_size: tamaño del batch que contiene las muestras seleccionadas aleatoriamente sin reemplazo.\n",
    "    - seed: semilla para garantizar la repetitividad del proceso.\n",
    "    '''\n",
    "    rng = np.random.default_rng(seed)\n",
    "    # Cantidad de frames a procesar\n",
    "    cant_frames = len(input_frames)\n",
    "    # Generación de enteros random\n",
    "    idx = rng.choice(cant_frames, size=batch_size, replace=False, shuffle=False)\n",
    "    # Armado del batch. Se convierte a tipo flotante\n",
    "    batch = input_frames[idx,...].astype(m_type)\n",
    "\n",
    "    # Mediana del batch\n",
    "    return np.median(batch, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foreground_naive(background, input_frame, thresh_value=0, max_val=255):\n",
    "    '''\n",
    "    Función que devuelve la máscara del objeto que se encuentra en movimiento. Incluye la binarizacion, empleando el método Otsu,\n",
    "    y la aplicación del filtro morfológico tipo cierre.\n",
    "    - background: tensor de 3 canales con la mediana del batch aleatorio de frames\n",
    "    - frame: frames de 3 canales con objetos a detectar.\n",
    "    - tresh_value: valor umbral del método OTSHU.\n",
    "    - max_val: valor máximo del método OTSHU.\n",
    "    '''\n",
    "    # Conversión del frame a float32\n",
    "    input_frame.astype('float32')\n",
    "    # resta\n",
    "    diff = background - input_frame\n",
    "    # normalizado\n",
    "    diff = cv.normalize(diff,None,0,255,cv.NORM_MINMAX).astype('uint8')\n",
    "    # pasaje a escala de grises\n",
    "    diff_gray = cv.cvtColor(diff,cv.COLOR_BGR2GRAY)\n",
    "    # binarización otsu\n",
    "    ret, thresh = cv.threshold(diff_gray,0,255,cv.THRESH_BINARY+cv.THRESH_OTSU)\n",
    "    # Creamos un elemento estructurante y aplicamos operaciones morfologicas\n",
    "    kernel = np.ones((3,3),np.uint8)\n",
    "    # Filtro morfológico tipo cierre.\n",
    "    closing = cv.morphologyEx(thresh, cv.MORPH_CLOSE, kernel, iterations = 3)\n",
    "\n",
    "    return closing\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruebas por frame<a name=\"id3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Selección de un frame del video: cambiar el número del primer índice\n",
    "# f_p = frames[100,...]\n",
    "# f_p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # background\n",
    "# background_p = background_naive(frames, 50, 10)\n",
    "# # foreground\n",
    "# mask = foreground_naive(background_p, f_p)\n",
    "\n",
    "# plt.imshow(mask, cmap='gray')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Implementación de máscara\n",
    "# indices = np.where(mask==255)\n",
    "# f_p[indices[0], indices[1], :] = [200, 0, 255]\n",
    "# plt.imshow(f_p)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentación de objetos en movimiento<a name=\"id4\"></a>\n",
    "A partir de las funciones presentadas, se segmentó el viceo de los elementos en movimiento a partir de la sustracción de fondo. En este caso particular se trató de personas. Los parámetros a definir previamente son:\n",
    "- El tamaño del batch de frames.\n",
    "- El tiempo de actualización del background.\n",
    "- El valor de la semilla para garantizar la repetitibilidad de la prueba. \n",
    "  \n",
    "Con los valores predefinidos, la implementación muestra buenos resultados de segmentación. Solo si se debe mencionar que no logra incluir la mayoría de los rostros en la segmentación. Ocurre también que, dependiendo el color de ropa, tampoco segmenta a la vestimenta en particular. Sin embargo son casos marginales y la figura principal en movimiento es detectada por completo. \n",
    "\n",
    "En cuanto a performance al correr el algoritmo, presenta el inconveniente de la actualización del background. Su ejecución presenta un tiempo muerto, igual al necesario para llevar a cabo la actualización del fondo del background.\n",
    "\n",
    "En la carpeta _result_ se encuentra grabado el video con el resultado final de la segmentación por substracción de fondo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros a definir para la segmentación\n",
    "# -----------------------------------------\n",
    "batch_size = 50 # tamaño del batch de frames para armar el background\n",
    "t = 10.0 # tiempo de muestreo\n",
    "seed = 122 # semilla para garantizar la repetitibilidad de la prueba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementación de la segmentación de objetos en movimiento por Naive\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "# Definición de las posiciones de frame donde se realiza la actualización del background\n",
    "update = np.arange(0, cant_frames, int(t*fps))\n",
    "update = np.append(update,1) # se agrega un 1 al final para que sean coherentes las dimensiones, no afecta a la implementación\n",
    "\n",
    "# Generación de semillas para garantizar la repetitibilidad de background generados aleatoriamente\n",
    "rng = np.random.default_rng(seed)\n",
    "seed_bg = rng.choice(4*len(update), size=len(update), replace=False, shuffle=False) \n",
    "\n",
    "# Contadores\n",
    "count=0\n",
    "s=0\n",
    "\n",
    "# Creación del objeto para aplicar Naive a los frames\n",
    "cap_naive = cv.VideoCapture(os.path.join(VD_DIR, VD_NAME))\n",
    "if not cap_naive.isOpened:\n",
    "    print('Falla al abrir el archivo: ' + VD_NAME)\n",
    "    exit(0)\n",
    "\n",
    "# Parámetros y objetos necesarios para grabar la ejecución (descomentar para grabación)\n",
    "# -------------------------------------------------------------------------------------\n",
    "# width = int(cap_naive.get(cv.CAP_PROP_FRAME_WIDTH))\n",
    "# height = int(cap_naive.get(cv.CAP_PROP_FRAME_HEIGHT))\n",
    "# size = (width, height)\n",
    "# fourcc = cv.VideoWriter_fourcc(*'XVID')\n",
    "# out_seg_naive = cv.VideoWriter(r'.\\result\\seg_naive.avi', fourcc, fps, size)\n",
    "\n",
    "\n",
    "# Ciclo de segmentación por substracción Naive\n",
    "# --------------------------------------------\n",
    "while True:\n",
    "    # Lectura de un frame\n",
    "    r_n, f_n = cap_naive.read()\n",
    "    if not r_n:\n",
    "        break\n",
    "    \n",
    "    # Actualización del background\n",
    "    if count == update[s]:\n",
    "        background = background_naive(frames, batch_size, seed_bg[s])\n",
    "        s+=1\n",
    "    # Contrucción del foreground \n",
    "    fg_n = foreground_naive(background, f_n)\n",
    "    # Aplicación de la máscara al frame que se está leyendo\n",
    "    idx_fg_n = np.where(fg_n==255)\n",
    "    f_n[idx_fg_n[0], idx_fg_n[1], :] = [200, 0, 255]\n",
    "    # Registro sobre la imagen del número de frame procesado\n",
    "    cv.rectangle(f_n, (10, 2), (100,20), (255,255,255), -1)\n",
    "    cv.putText(f_n, str(cap_naive.get(cv.CAP_PROP_POS_FRAMES)), (15, 15),\n",
    "               cv.FONT_HERSHEY_SIMPLEX, 0.5 , (0,0,0))\n",
    "\n",
    "    # Salida foreground y frame segmentado\n",
    "    cv.imshow('Foreground Naive', fg_n)\n",
    "    cv.imshow('Segmentado Naive', f_n)\n",
    "    # Grabación (descomentar para grabar)\n",
    "    # out_seg_naive.write(f_n)\n",
    "    \n",
    "    count+=1\n",
    "\n",
    "    # Se corre el video hasta que termine o se apriete escape. Aprentando 's' se guarda el frame actual\n",
    "    keyboard = cv.waitKey(30)\n",
    "    if keyboard == 'q' or keyboard == 27:\n",
    "        break\n",
    "    elif keyboard == ord('s'):\n",
    "        cv.imwrite(r'\\result\\frame_seg_naive.png',f)\n",
    "\n",
    "cv.destroyAllWindows()\n",
    "# Descomentar si se realiza grabación\n",
    "# out_seg_naive.release()\n",
    "cap_naive.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparativa con el método de mezclas gaussianas<a name=\"id5\"></a>\n",
    "Se aplica el algoritmo **MOG2** al video. Se basa en la segmentación de fondo/primer plano basado en una mezcla de gaussianas. Este se encarga de seleccionar el número apropiado de distribución gaussiana para cada píxel. También MOG utiliza un método para modelar cada píxel de fondo mediante una mezcla de distribuciones K gaussianas (K es de 3 a 5). MOG2 selecciona el número apropiado de gaussianas para cada pixel automáticamente. \n",
    "\n",
    "En cuanto a resultados, se tiene que:\n",
    "- La calidad de segmentación no fue tan buena como con la implementación de Naive.\n",
    "- La performance de la ejecución del algoritmo si resultó mucho mejor. No se presenta el inconveniente del tiempo muerto por actualización del background.\n",
    "\n",
    "En la carpeta _result_ se encuentra grabado el video con el resultado final de la segmentación aplicando MOG2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación del objeto para aplicar Naive a los frames\n",
    "cap_MOG2 = cv.VideoCapture(os.path.join(VD_DIR, VD_NAME))\n",
    "if not cap_MOG2.isOpened:\n",
    "    print('Falla al abrir el archivo: ' + VD_NAME)\n",
    "    exit(0)\n",
    "\n",
    "# Parámetros y objetos necesarios para grabar la ejecución (descomentar para grabación)\n",
    "# Se debe hacer una grabación de video por vez\n",
    "# -------------------------------------------------------------------------------------\n",
    "# width = int(cap_MOG2.get(cv.CAP_PROP_FRAME_WIDTH))\n",
    "# height = int(cap_MOG2.get(cv.CAP_PROP_FRAME_HEIGHT))\n",
    "# size = (width, height)\n",
    "# fourcc = cv.VideoWriter_fourcc(*'XVID')\n",
    "# out_seg_MOG2 = cv.VideoWriter(r'.\\result\\seg_MOG2.avi', fourcc, fps, size)\n",
    "\n",
    "# Creación del objeto openCV para substracción de fondo por MOG2\n",
    "backSub = cv.createBackgroundSubtractorMOG2()\n",
    "\n",
    "# Ciclo de segmentación por susbtracción MOG2\n",
    "#--------------------------------------------\n",
    "while True:\n",
    "    # Lectura de un frame\n",
    "    r_MOG2, f_MOG2 = cap_MOG2.read()\n",
    "    if not r_MOG2:\n",
    "        break\n",
    "    \n",
    "    # Construcción del foreground\n",
    "    fg_MOG2 = backSub.apply(f_MOG2)\n",
    "    # Aplicación de la máscara al frame que se está leyendo\n",
    "    idx_fg_MOG2 = np.where(fg_MOG2==255)\n",
    "    f_MOG2[idx_fg_MOG2[0], idx_fg_MOG2[1], :] = [200, 0, 255]\n",
    "    # Registro sobre la imagen del número de frame procesado\n",
    "    cv.rectangle(f_MOG2, (10, 2), (100,20), (255,255,255), -1)\n",
    "    cv.putText(f_MOG2, str(cap_MOG2.get(cv.CAP_PROP_POS_FRAMES)), (15, 15),\n",
    "               cv.FONT_HERSHEY_SIMPLEX, 0.5 , (0,0,0))\n",
    "\n",
    "    # Salida foreground y frame segmentado\n",
    "    cv.imshow('Foreground MOG2', fg_MOG2)\n",
    "    cv.imshow('Segmentado MOG2', f_MOG2)\n",
    "    # Grabación (descomentar para grabar)\n",
    "    # out_seg_MOG2.write(f_MOG2)\n",
    "    \n",
    "    # Se corre el video hasta que termine o se apriete escape. Aprentando 's' se guarda el frame actual\n",
    "    keyboard = cv.waitKey(30)\n",
    "    if keyboard == 'q' or keyboard == 27:\n",
    "        break\n",
    "    elif keyboard == ord('s'):\n",
    "        cv.imwrite(r'\\result\\frame_seg_MOG2.png',f)\n",
    "\n",
    "cv.destroyAllWindows()\n",
    "# Descomentar si se realiza grabación\n",
    "# out_seg_MOG2.release()\n",
    "cap_MOG2.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('opencv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c41c2bed6317fcc09dc6ef78ff357819e2ca7efa781e228fd0e70c6f824fa93"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
